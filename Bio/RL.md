
# TensorFlow + Keras

## Outline / Reading List

- [Optimization Bayesian Learning Approximated](https://arxiv.org/abs/1906.02506)
  - [Video](https://slideslive.com/38923183/deep-learning-with-bayesian-principles) from [Mohammad Emtiyaz Khan](https://emtiyaz.github.io/)
- [VAEs](https://arxiv.org/abs/1312.6114)
  - [Semi-Supervised Learning](https://arxiv.org/abs/1406.5298)
  - [Normalizing Flows](https://arxiv.org/abs/1908.09257)
    - [Semi-Supervised Learning with Normalizing Flows](https://arxiv.org/abs/1912.13025)
    - [Masked Autoregressive Flows](https://arxiv.org/abs/1705.07057)
- [LeNet-5](http://yann.lecun.com/exdb/lenet/)
- [Advantages of Bayesian Deep Learning](https://arxiv.org/abs/2001.10995)
  - [Bayes by Backprop](https://arxiv.org/abs/1505.05424)
    - and the key [reference](https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks) therein
  - [Bayesian Dropout](https://arxiv.org/abs/1506.02142)
    - [Concerns](https://www.semanticscholar.org/paper/Risk-versus-Uncertainty-in-Deep-Learning-%3A-Bayes-%2C-Osband/dde4b95be20a160253a6cc9ecd75492a13d60c10) and [Comments](https://www.reddit.com/r/MachineLearning/comments/7bm4b2/d_what_is_the_current_state_of_dropout_as/),
    [ect.](https://www.reddit.com/r/MachineLearning/comments/8w0v9m/d_ian_osband_dropout_posteriors_give_bad/)
  - [Randomized Prior Functions](https://arxiv.org/abs/1806.03335)
    - [great implementation example](https://gdmarmerola.github.io/risk-and-uncertainty-deep-learning/)
- [BNN posteriors are probably wrong](https://arxiv.org/abs/1906.09686)
- [HMC is the gold standard](https://arxiv.org/abs/1701.02434)
  - [NUTS is the gold standard HMC](https://arxiv.org/abs/1111.4246)
- [Bayesian Surprise](https://www.sciencedirect.com/science/article/abs/pii/S0378375802002823)
  - - and the key [reference](https://www.jstor.org/stable/2685531?seq=1) therein